---
title: "GROUP3"
output:
  html_notebook:
    number_section: yes
  html_document:
    df_print: paged
---

```{r setup, include=TRUE, echo=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::knit_hooks$set(document = function(x) {
    paste(rapply(strsplit(x, '\n'), function(y) Filter(function(z) !grepl('# HIDE',z),y)), collapse ='\n')
})
```

```{r libraries, echo = FALSE}
library(factoextra)
library(kableExtra)

library(FactoMineR)

library(edgeR)

library(AnnotationDbi)
library(GO.db)
library(org.Hs.eg.db)

library(EDASeq)

library(cluster)

library(caret)
library(e1071)
```

```{r load functions, echo=FALSE}
source(paste(getwd(), '/src/utilities.R', sep = ""))
source(paste(getwd(), '/src/functions.R', sep = ""))
```

# First Part

## Load the data in R

```{r load data, results='hold', collapse=TRUE}
{# HIDE
DATA   <- read.table(getRawPath("E-GEOD-76987-raw-counts.tsv"), sep = "\t", row.names = 1, header = TRUE)
LABELS <- read.delim(getRawPath("labels.txt"), sep = "\t", header = TRUE)
}# HIDE
```

## Calculate the sequencing depth of each sample

```{r calculate depth, results='hold', collapse=TRUE}
# extract information about data
genes <- DATA[,1]                
samples <- DATA[,2:ncol(DATA)]
genes_number <- length(genes)         
samples_number <- length(samples)   

# Computing the sequencing depth of each sample

depth <- apply(samples, 2, sum)     # 2 means that we're appling the sum by columns

{# HIDE
cat("Our dataset is composed by:\n\n")
cat(paste("♦", genes_number, "genes\n", sep = " "))           
cat(paste("♦", samples_number, "samples\n\n", sep = " "))

cat("The sequencing depth of each sample is:\n", depth)

cat("\nThe average depth is:", mean(depth))
}# HIDE
```

## Produce the MvA plots of each sample vs. sample 1 

<div style="text-align: justify">
We decided to use the first sample as reference because we noticed that there was not any significant difference between plots with other references. We added 1 to each value in the data matrix to avoid the log(0) computation in the following analysis.
</div>

```{r generate temp dataset}
temp_samples <- samples + 1
#TODO: save the temporary datasets into data/temp and reload it

extracted_index <- 1
    
interval <- (1:samples_number)[-extracted_index]
```

<div style="text-align: justify">
The following function computes the intensity ratio and the average intensity of the vectors distribution in input by definition and return a list of them. The second function receives in input a matrix, the reference column index, the interval used to scan other samples, eventually a folder in which plots will be saved and the graph title. It computes M and A thanks to the MA function and plots the results.
</div>

```{r MA function}
MA
```

```{r produceMvA function}
produceMvA
```

```{r produce MvA plots, eval = FALSE}
A <- matrix(0, nrow = genes_number, ncol = 0)
M <- matrix(0, nrow = genes_number, ncol = 0)

produceMvA(temp_samples, extracted_index, interval,  folder = "MvA", graph_title = "MvA Plot")
producePlots(temp_samples, extracted_index, interval, folder = "Other")
```


## TO DO: COMMENTARE PCA 


```{r compute PCA, results='hide'}
# PCA for samples
temp_pca <- PCA(t(temp_samples), scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point', title = 'PCA for samples')

# PCA for genes
temp_pca <- PCA(temp_samples, scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point', title = 'PCA for genes')
```

## Implement the TMM and the quantile normalization

<div style="text-align: justify">
The following function implements the Trimmed-Mean Normalization. It receives in input a matrix x with samples in columns and genes in rows, a reference index of the reference subject with respect to which we perform normalization and an interval through which we scan all the other genes. For each gene except for the reference, it computes the MA values with respect to that reference, calculates the scaling factor as the trimmed mean of M values and does the normalization adding the scaling factor as exponential in base 2 to the current gene. 
</div>

```{r TMM Normalization Implementation}
tmm_normalization
```

<div style="text-align: justify">
The following function implements the Quantile Normalization. It receives in input the matrix x with samples in columns and genes in rows for which it computes the quantile normalization as defined in theory lessons. The setup matrixes for indexes, sorted values and for normed values are created; then, for each subject, the gene raw counts of that subject are sorted and given a rank usign the average method to deal with ties. Ranks will be used as indexes. Then, the mean by row is calculated on the matrix with sorted columns. At last,column by column,  the normalized matrix is built using the indexes from the ranking function to choose which mean values should be put in which positions. 
The quantile normalization return a matrix whose columns have all the same distribution. 
</div>

```{r Quantile Normalization Implementation}
quantile_normalization
```

## Normalize the data using the method of your choice

```{r TMM Normalization}

A <- matrix(0, nrow = genes_number, ncol = 0)
M <- matrix(0, nrow = genes_number, ncol = 0)

tmm_normed <- tmm_normalization(temp_samples, extracted_index, interval)
```

```{r TMM plot}
par(mfrow=c(1,2)) 
extracted <- tmm_normed$samples[,1]
selected  <- tmm_normed$samples[,12]

plot(samples[,1], samples[,12], xlab="Sample 1", ylab="Sample 12", main = "Samples plot")

plot(extracted, selected, xlab="Sample 1", ylab="Sample 12", main = "Samples plot - Normalized")
```

```{r Quantile normalization}
A <- matrix(0, nrow = genes_number, ncol = 0)
M <- matrix(0, nrow = genes_number, ncol = 0)

quantile_normed <- quantile_normalization(temp_samples)
```

```{r Quantile plots}
par(mfrow=c(1,2)) 
extracted <- quantile_normed$samples[,1]
selected  <- quantile_normed$samples[,12]

plot(samples[,1], samples[,12], xlab="Sample 1", ylab="Sample 12", main = "Samples plot")

plot(extracted, selected, xlab="Sample 1", ylab="Sample 12", main = "Samples plot - Normalized")
```

<div style="text-align: justify">
With the results obtained, we decided to use for the subsequent analysis the <strong>quantile normalization</strong> of the data; in fact, this type of normalization seems more robust and gives better results. If we look at the MvA plots of sample 1 (the reference) VS sample 12, we can see that in the quantile normalization plot the cloud of points is more or less lying on M=0, as it should, while in the TMM normalization plot the cloud of points is clearly hanging below M=0. 
It is to be noted that neither the quantile nor the TMM normalization get rid of the "V-shaped" outliers near low values of A (average). The code below shows what was said here. 
</div>

```{r MvA plot of reference sample (sample 1) VS sample 12 with TMM and quantile normalization}
# Quantile normalization: data extraction and plot 
par(mfrow=c(1,2)) 
extracted <- quantile_normed$samples[,1]
selected  <- quantile_normed$samples[,12]

computed  <- MA(extracted, selected)
    
M <- cbind(M, computed$M)
A <- cbind(A, computed$A) 
    
plot(computed$A, computed$M, xlab="A", ylab="M", main = "MvA plot - Quantile", sub = paste("Sample", 1, "vs.", 12, sep = " "))
abline(0,0)

# TMM normalization: data extraction and plot 
extracted <- tmm_normed$samples[,1]
selected  <- tmm_normed$samples[,12]
computed  <- MA(extracted, selected)
    
M <- cbind(M, computed$M)
A <- cbind(A, computed$A)
    
plot(computed$A, computed$M, xlab="A", ylab="M", main = "MvA plot - TMM ", sub = paste("Sample", 1, "vs.", 12, sep = " "))
abline(0,0)
```

## Produce the MvA plots of each sample vs. sample 1 using normalized data to evaluate if the normalization step was correctly performed or if there are outlier samples

```{r Produce Normalized MvA Plots, eval = FALSE}
# TMM Normalization 
produceMvA(tmm_normed$samples, extracted_index, interval,  folder = "MvA - TMM Normalization", graph_title = "MvA (TMM Normalization)")

# Quantile Normalization
produceMvA(quantile_normed$samples, extracted_index, interval,  folder = "MvA - Quantile Normalization", graph_title = "MvA (Quantile Normalization)")

```

<div style="text-align: justify">
You can see MvA plots inside output/plots folder and in particular you can notice that in some of them it's clear what was explained above with reference sample vs. sample 12. For example in reference sample vs. sample 47 plot the same comments can be made: MvA plots clearly show a deviance from M=0 in TMM normalization while this deviance is not present in the analysis with the quantile normalization. This gives credit to the idea that quantile normalization is more robust because it produces a series of sample with the same distribution, while TMM normalization just takes care of the most evident outliers, because it uses a trimmed mean approach.
</div>


## TO DO - COMMENTARE PCA

```{r Compute normalized PCA, results='hide'}
# PCA for samples
temp_pca <- PCA(t(tmm_normed$samples), scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point', title = 'PCA for samples after TMM normalization')

temp_pca <- PCA(t(quantile_normed$samples), scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point' , title = 'PCA for samples after quantile normalization')

# PCA for genes
temp_pca <- PCA(tmm_normed$samples, scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point', title = 'PCA for genes after TMM normalization')

temp_pca <- PCA(quantile_normed$samples, scale.unit = FALSE, ncp = 5, graph = FALSE)
# summary(temp_pca)
fviz_pca_ind(temp_pca, col.var = 'cos2', geom = 'point', title = 'PCA for genes after quantile normalization')
```

# Second Part

## [Note] There can be the same sample measured twice for a specific subject

<div style="text-align: justify">
We decided to take care of this issue by simply using a mean approach: the samples coming from the same subject and present in the same group (for instance subject 1 is measured two times as a control subject) are mediated. To do this task we decided to implement the remove_duplicates function. 
The function:
<ul>
<li>first of all, we find all subjects duplicated in the group (in our case the group can be "control" or "disease"). To do so, we use the column of the group "individuals"; this column has the identifier for the subjects. If a sample comes from the same subject (i.e, the subject has benn sampled twice or more, so it is a duplicate), the identifier will be the same.</li>
<li>We save all these dupes in a temporary matrix called "duplicate_group"</li>
<li>we initialize a new matrix which will contain all subjects without any of them being duplicated (i.e., sampled twice)</li>
<li>for each identifier in "duplicate_group", we find the indexes of this duplicate individual and get their SRR code (the code with which samples are identified in the matrix of raw data). NB: in the matrix of raw data, all samples are identified with different SRR codes, but SRR codes can point to the same subject.</li>
<li>get all the the samples coming from this particular subject that we know is duplicated</li>
<li>apply the mean by row to all samples identified in the previous step and save this new "mediated-samples subject" in the matrix initialized above</li>
<li>then, find all subjects in the group that are not duplicated (i.e., there's a 1 to 1 correspondance of subject to sample for these individuals)</li>
<li>put all these subjects not duplicated into the matrix initialized above</li>
<li>remove from the matrix any columns not used (we initialized the matrix having the same columns as the group but obviously in the end it will have less columns because some samples will not be present anymore)</li>
end.
</ul>
</div>

```{r remove duplicates function}
remove_duplicates
```

```{r removal of the duplicates}
# get the 'normal' and 'uninvolved mucosa' samples (samples of the control group) 
normal <- LABELS[LABELS$sample_type == c("normal"),]            
unimuc <- LABELS[LABELS$sample_type == c("uninvolved mucosa"),] 

# concatenate and sort samples
control <- rbind(normal, unimuc)                               
control <- control[order(as.numeric(control$individual)),]    

# get the 'colon sessile serrated adenoma/polyp' samples (samples of the disease group)
disease <- LABELS[LABELS$sample_type == c("colon sessile serrated adenoma/polyp"),]

# remove duplicates in the groups
control_nodup <- remove_duplicates(quantile_normed$samples, control)
disease_nodup <- remove_duplicates(quantile_normed$samples, disease) 
```

```{r remove_duplicates outputs}
{# HIDE
# some outputs
cat("The number of control samples is:",nrow(control))
cat("\nThe number of diseased samples is:",nrow(disease))

cat("\nThe number of control samples after duplicates removal is:",ncol(control_nodup))
cat("\nThe number of diseased samples after duplicates removal is:",ncol(disease_nodup))

ratio <- 1-(ncol(control_nodup)/nrow(control))
cat("\nThe compression ratio for control has been of:", ratio)
ratio <- 1-(ncol(disease_nodup)/nrow(disease))
cat("\nThe compression ratio for disease has been of:", ratio)
}# HIDE
```

## [Note] There can be some genes that in both groups 1 and 2 have always expression equal to 0

<div style="text-align: justify">
We took care of the zero problem by taking out genes that had expression always equal to zero in all samples from the control group and the disease group. For instance, say that we look at gene 1: if the sum of the read counts for gene 1 in all the samples coming from control group is zero as it is in all the samples from the disease group then we can say that gene 1 has always expression zero, so it does not give information about differentially expressed genes in disease VS control groups. Gene 1 in this example is therefore taken out. We implemented this in the remove_zeros function.
</div>

```{r remove zero function}
remove_zeros
```

```{r taking care of zeros}
groups_nozero <- remove_zeros(control_nodup, disease_nodup)
control_nodup_nozero<-groups_nozero$control
disease_nodup_nozero<-groups_nozero$disease
```

```{r remove_zeros outputs}
{# HIDE
# some outputs
cat("The number of initial genes is:",nrow(control_nodup))
cat("\nThe number of genes after zeros removal for control is:",nrow(control_nodup_nozero))

ratio <- 1-(nrow(control_nodup_nozero)/nrow(control_nodup))
cat("\nThe compression ratio for control has been of:", ratio)
}# HIDE
```

## Calculate p-values of DE analysis (not corrected for multiple testing) between the two groups using t-test, Wilcoxon test and edgeR

<div style="text-align: justify">
For the t-test and the Wilcoxon tests, we use functions of the Stats library.
</div>

```{r p-values}
#initialization of the variables
Nc <- nrow(control_nodup_nozero)
c_ttest_pvalue <- NULL
c_wilcoxon_pvalue <- NULL

# use of the t.test and wilcoxon.test functions for each gene
for(i in (1:Nc)){ 
  c_ttest_pvalue <- c(c_ttest_pvalue,t.test(control_nodup_nozero[i,], disease_nodup_nozero[i,], var.equal = FALSE)$p.value)
  c_wilcoxon_pvalue <- c(c_wilcoxon_pvalue,wilcox.test(control_nodup_nozero[i,],disease_nodup_nozero[i,], exact=FALSE)$p.value)
}
```

<div style="text-align: justify">
For edgeR, we have to do a preprocessing of data and then we use functions of the edgeR library. We have to rebuild the control and disease matrixes with data not normalized because edgeR was created to work with data whose distribution is a negative binomial: the initial data matrix has this type of distribution, while normalized data does not comply to negative binomial distribution. To correctly use the functions we have to label samples in control and disease groups with 'control_1', 'control_2', ..., 'disease_1', and so on. This is done by the utility function rename_columns.
</div>

```{r rename column function}
renameColumns
```

```{r preprocessing}
# rebuilt control matrix and label with the proper name
control_nodup <- remove_duplicates(DATA, control)
control_nodup <- renameColumns(control_nodup, "control")

# rebuilt disease matrix and label with the proper name 
disease_nodup <- remove_duplicates(DATA, disease)
disease_nodup <- renameColumns(disease_nodup, "disease")
```

<div style="text-align: justify">
The next chunk implements edgeR in order to get the pvalues of this test. It is to be noted that the test uses a generalized linear regression approach.
</div>

```{r edgeR}
# merge the two tables of disease and control 
mat <- cbind(disease_nodup, control_nodup)

# create groups and merge them
control_group <- rep("control",dim(control_nodup)[2])
disease_group <- rep("disease",dim(disease_nodup)[2])
group_all <- cbind(t(as.data.frame(disease_group)),t(as.data.frame(control_group)))

# with factor() we get an object divided in two levels (control and disease)
group <- factor(group_all)

# design matrix
design <- model.matrix(~0+group) 
rownames(design) <- colnames(mat)

# fit values of phi (step to fit our GLM model)
y <- DGEList(counts=mat, remove.zeros = TRUE)    
y <- calcNormFactors(y)   # scaling factors with TMM approach
SF <- y$samples

y <- estimateGLMCommonDisp(y,design, verbose=TRUE) # phi common to the entire dataset
y <- estimateGLMTrendedDisp(y,design) # phi depends on mu
y <- estimateGLMTagwiseDisp(y,design) # phi is gene specific
fit <- glmFit(y,design) # the model fit 

# the test
Confr <- makeContrasts(Treatment=groupdisease-groupcontrol,levels=design)
RES <- glmLRT(fit,contrast=Confr[,"Treatment"])

# some outputs to give an idea of the test's results
RES$table[1:5,]

# final values
out <- topTags(RES, n = "Inf")$table
out[1:5,]
```

<div style="text-align: justify">
Boxplots of the p-values obtained by the three tests are shown.
P-values from Wilcoxon test and p-values obtained by edgeR are very similar in their distributions, while the distribution of p-values obtained by t-test differ the most from the other two.
</div>

```{r plot p-values}
boxplot(c_ttest_pvalue, c_wilcoxon_pvalue, out$PValue, names = c('t-test','Wilcoxon test', 'edgeR'), main = 'p-values')
```

## Calculate the expected number of false positives (FP) and false negatives (FN) in correspondence to the choice of alpha = 0.05 and consider G0 = N with N being the number of genes

<div style="text-align: justify">
For each test, we extracted the genes that have p-value lower than a fixed alpha. These are the selcted genes.
With the following function "expected_values" we calculate the expected values of true positives, false positives, true negatives and false negatives implementing their definition. In order of calculation:
<ul>
<li>False positives are calculated as the minimum between G0*alpha and the number of selected genes,</li>
<li>True positives are calculated as the maximum between zero and the number of selected genes minus the number of expected positives,</li>
<li>True negatives as G0 minus the number of expected false positives,</li>
<li>and lastly, False negatives as the maximum between zero and the total number of genes minus the selected ones and minus the number of expected true negatives; in this case, the maximum is applied as not to have deviant values for FN, meaning negative values.</li>
</ul>
We then applied the function considering G0 (number of selected genes) being equal to G, the total number of genes.
</div>

```{r selection for alpha 0.05}
alpha<-0.05

# selected values for t-test
lower <- (c_ttest_pvalue<alpha)
selected_ttest <- which(lower==TRUE)
num_sel_ttest <- length(selected_ttest)

# selected values for Wilcoxon test
lower <- (c_wilcoxon_pvalue<alpha)
selected_wilcox <- which(lower==TRUE)
num_sel_wilcox <- length(selected_wilcox)

#selected values for edgeR
num_sel_edgeR <- length(which(out$PValue<alpha)) #i selected

{# HIDE
cat("Results")
cat("\nNumber of selected genes with t-test:",num_sel_ttest)
cat("\nNumber of selected genes with Wilcoxon-test:",num_sel_wilcox)
cat("\nNumber of selected genes with edgeR:",num_sel_edgeR)
}# HIDE
```

```{r expected values function}
expected_values
```

```{r calculation of expected values}
G <- nrow(control_nodup_nozero)
G0 <- G

#function that returns a vector with, in order, TP FP TN FN
expected_ttest <- expected_values(G, G0, alpha, num_sel_ttest)
expected_wilcoxontest <- expected_values(G, G0, alpha, num_sel_wilcox)
expected_edger <- expected_values(G, G0, alpha, num_sel_edgeR)

dt <- rbind(expected_ttest,expected_wilcoxontest,expected_edger)
dt <- cbind(c(G0,G0,G0),dt)
rownames(dt)<-c('t-test', 'Wilcoxon-test', 'edgeR')
colnames(dt)<-c('G0 = G','TP', 'FP', 'TN', 'FN')
dt %>% kbl() %>% kable_paper("hover", full_width = T)
```

## Estimate the number of not differentially expressed genes G0 and re-estimate the expected number of false positives and false negatives

<div style="text-align: justify">
The first function calculates the values of G0 for a given set of lambda values. It's used to estimate the correct value of G0 rather than assuming that G0 is equal to the total number of genes. It receives in input also the p-values for which the computation must be done, the total number of genes G and the filename in which the plot of "G0 VS lambda" should be saved. From the plot of G0/G VS lambda we choose by hand the best value for our lambda (for instance, in the case of edgeR, 0.65) and with the second function, we provide an estimate of G0 with the previous lambda value passed as input (considering a mean of the values of G0 within an interval around the best lambda; the interval is chosen by epsilon).  
</div>

```{r G0 values function}
G0values
```

```{r estimation of G0 function}
G0_value_estimation
```

<div style="text-align: justify">
In the following chunk we perform the estimation of G0 with the functions described above; after seeing the plots, we choose our lambdas in the three cases (the three tests); those lambdas were chosen because we saw that around them the respective graphs had a constant mean value with little variability.
</div>

``` {r estimation of G0 and re-estimation of the values for each test}
lambda<-seq(0, 0.99, 0.01)

# t-test analysis
res <- G0values(lambda,c_ttest_pvalue, G0, "T test")
lambda_est_ttest <- 0.8
eps <- 0.03
G0_est_ttest <- G0_value_estimation(lambda_est_ttest, eps, res)

expected_ttest_est <- expected_values(G, G0_est_ttest, alpha, num_sel_ttest)

# Wilcoxon analysis
res <- G0values(lambda,c_wilcoxon_pvalue, G0, "Wilcoxon test")
lambda_est_wilcoxon <- 0.8
eps <- 0.03
G0_est_wilcoxontest <- G0_value_estimation(lambda_est_wilcoxon, eps, res)

expected_wilcoxontest_est <- expected_values(G, G0_est_wilcoxontest, alpha, num_sel_wilcox)

# edgeR analysis
res <- G0values(lambda,out[,4], G0, "edgeR test")
lambda_est_edger <- 0.65
eps <- 0.03
G0_est_edger <- G0_value_estimation(lambda_est_edger, eps, res)

expected_edgeR_est <- expected_values(G, G0_est_edger, alpha, num_sel_edgeR)
```

<div style="text-align: justify">
In the table below, we provide the results of the three tests in terms of FP, FN, TP, TN rates; moreover, we address sensitivity, specificity and accuracy of these tests. For better understanding, this ananlysis is done ultimately only for the results obtained by estimating G0.
From the table we can see that although having slightly different FP, FN, TP, TN rates, the three tests have the same specificity, defined as TN/(TN+FP). This is not true for sensitivity: edgeR is the best test in terms of sensitivity defined as TP/(TP+FN). The same can be said about accuracy, calculated as (TP+TN)/(TP+TN+FP+FN); edgeR is still the best performing test.
</div>

```{r sensitivity, specificity, accuracy}
sensitivity <- c((expected_ttest_est$TP/(expected_ttest_est$TP+expected_ttest_est$FN)),
                 (expected_wilcoxontest_est$TP/(expected_wilcoxontest_est$TP+expected_wilcoxontest_est$FN)),
                 (expected_edgeR_est$TP/(expected_edgeR_est$TP+expected_edgeR_est$FN)))

specificity <- c((expected_ttest_est$TN/(expected_ttest_est$TN+expected_ttest_est$FP)),
                 (expected_wilcoxontest_est$TN/(expected_wilcoxontest_est$TN+expected_wilcoxontest_est$FP)),
                 (expected_edgeR_est$TN/(expected_edgeR_est$TN+expected_edgeR_est$FP)))

accuracy <- c(((expected_ttest_est$TN+expected_ttest_est$TP)/(expected_ttest_est$TP+expected_ttest_est$TN+expected_ttest_est$FP+expected_ttest_est$FN)),
              ((expected_wilcoxontest_est$TN+expected_wilcoxontest_est$TP)/(expected_wilcoxontest_est$TP+expected_wilcoxontest_est$TN+expected_wilcoxontest_est$FP+expected_wilcoxontest_est$FN)),
              ((expected_edgeR_est$TN+expected_edgeR_est$TP)/(expected_edgeR_est$TP+expected_edgeR_est$TN+expected_edgeR_est$FP+expected_edgeR_est$FN)))
```

<div style="text-align: justify">
The table of results.
</div>

```{r results, echo=FALSE}
dt <- rbind(expected_ttest_est,expected_wilcoxontest_est,expected_edgeR_est)
dt <- cbind(c(G0_est_ttest,G0_est_wilcoxontest,G0_est_edger),dt,sensitivity,specificity,accuracy)
rownames(dt)<-c('t-test', 'Wilcoxon-test', 'edgeR')
colnames(dt)<-c('G0 estimated','TP', 'FP', 'TN', 'FN','Sensitivity','Specificity', 'Accuracy')
kbl(dt) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

#knitr::kable(dt, align = "lccccrrr", table.attr = "style='width:30%;'")
```

## Choose the “best” test among t-test, Wilcoxon test and edgeR motivating your choice

<div style="text-align: justify">
We chose edgeR for few reasons; first of all, edgeR implements a test for data with negative binomial distribution, while the other tests do not take into account the characteristics of this particular distribution and, in this sense, this analysis can give more specific results. Then, it must also be said that it gives the lowest FN rate; while FP rate can be easily taken into account with a correction for multiple tests, FN rate is not accounted for with this type of correction. In this sense, we are not able to reduce this rate with some corrections; since ideally one should want a FN and FP rates equal to zero, we have to minimize them. Finally, edgeR performs better than the other two tests in terms of both sensitivity and accuracy. 
To conclude, we chose edgeR on the basis of the theory behind it and also on the calculated FP and FN rates, sensitivity and accuracy.
</div>

## Select the final list of DE genes using the test chosen at point 4 and a false discovery rate threshold of 5%

<div style="text-align: justify">
The next chunk performs the FDR estimation to correct for multimuple testing.
First of all, we compute the lambda values choosing an interval within the minimum and maximum values of edgeR's p-values;for each of them we compute FDR value by definition as the expected number of false positives divided by the number of selected for said value of lambda. Then, we estimate the alpha value as the mean of alphas which have FDR close to 0.05 by a certain epsilon (0.0001).
</div>

```{r choose alpha}
FDR <- 0.05
# values in the range observed as p-value in edgeR
lambda <- seq(min(out[,4]), max(out[,4]), (max(out[,4])-min(out[,4]))/nrow(out))
FDR_values <- NULL

# compute FDR for every lambda
for (i in (1:length(lambda))) {
  less <- (out[,4]<lambda[i])
  num_sel <- length(which(less==TRUE))
  
  expected_val <- expected_values(G, G0_est_edger, lambda[i], num_sel)
  
  if (num_sel==0) FDR_values <- c(FDR_values, 0)
  else FDR_values <- c(FDR_values, (expected_val$FP/num_sel))
  
}

plot(lambda, FDR_values, main = 'FDR values for different alpha')

# choose the values that are in [0.05-epsilon;0.05 + espilon]
epsilon <- 0.0001
alpha_idx_lower <- which(FDR_values >= FDR - epsilon)
alpha_idx_upper <- which(FDR_values <= FDR + epsilon)
alpha_est <- mean(lambda[intersect(alpha_idx_lower, alpha_idx_upper)])
```

<div style="text-align: justify">
We finally select the genes with p-value given by edgeR lower than the estimated alpha.
</div>

```{r extraction}
# selection of the correspondent indexes and ID genes in the table
indexes <- which(out$PValue<alpha_est) 
index_genes_selected <- sort(as.numeric(rownames(out[indexes,])))

ID_genes_selected <- rownames(DATA[index_genes_selected,])
number_genes_selected <- length(ID_genes_selected)
ID_genes_notselected <- setdiff(rownames(DATA),ID_genes_selected)
number_genes_notselected <- length(ID_genes_notselected)

{# HIDE
# print results
cat("Number of selected genes: ", number_genes_selected, "\n")
cat("Number of not selected genes: ", number_genes_notselected)
}# HIDE

```

# Third Part

## Extract G0 term associated

<div style="text-align: justify">
We found out that some gene names were duplicated but not their ENSEMBL IDs; for this reason, we decided to use these IDs to get the annotation and extract the right GO terms associated with the genes. We also decided to take out those genes whose GOALL was NA: as a matter of fact, this NA value means that we don't have their associated term right now (the association is not complete in the DB).  To improve computation, we also took out all the IDs of genes annotated but not selected and their respective total numbers. 
After the data extraction and the pre-processing, we built a matrix that has on rows the associated GO terms and on columns the type of GOterm (BP, CC, MF) and the values useful for the computation of the fisher test.
</div>

```{r extract GO terms}
# extraction of the associated terms in function of the ENSEMBL ID 
alldata <- select(org.Hs.eg.db, ID_genes_selected, columns = c("SYMBOL","ENTREZID", "ENSEMBL","GOALL"), keytype="ENSEMBL")

# remove the genes for which we have the NA term associated 
GOALL_NA <- which(is.na(alldata$GOALL))
ID_goall_na <- alldata$ENSEMBL[GOALL_NA]

ID_genes_selected_notna <- setdiff(ID_genes_selected,ID_goall_na)
number_genes_selected_notna <- length(ID_genes_selected_notna)
ID_genes_notselected_notna <- setdiff(ID_genes_notselected,ID_goall_na)
number_genes_notselected_notna <- length(ID_genes_notselected_notna)

# remove the duplicates in the extracted terms 
terms <- unique(alldata[,4])
terms <- terms[!is.na(terms)]

{# HIDE
# print outputs
cat("Number of unique extracted terms: ", length(terms), "\n")
cat("Number of selected genes not NA: ", number_genes_selected_notna)
}# HIDE

#creation of the matrix for the Fisher test, one row for each GOterm
matrixes <- NULL
for (i in (1:length(terms))){
  GOterm <- terms[i]
  GOterm_indexes <- which(alldata$GOALL==GOterm)
  a <- length(intersect(alldata[GOterm_indexes,1],ID_genes_selected_notna))
  b <- number_genes_selected - a
  c <- length(GOterm_indexes) - a
  d <- length(ID_genes_notselected_notna)- c
  type<-alldata[(which(alldata[,4]==GOterm))[1],6]

  matrixes <- rbind(matrixes,c(type,a,b,c,d))
}

colnames(matrixes)<-c("type","a","b","c","d")
rownames(matrixes)<-terms
matrixes<-matrixes[order(rownames(matrixes)),]
```

## Fisher test

<div style="text-align: justify">
These functions were implemeted to perform the fisher exact test.
"fisher_test_matrixes" computes the actual test, returning the p-values of each test.
"FDR_fisher" calculates the estimated alpha given a FDR equal to 0.05 in order to account for multiple testing in the selection of meaningful p-values (and subsequently meaningful annotations).
"annotation_terms" return the annotated terms for the GOterms selected as meaningful; to do so it uses the GO.db databases. 
</div>

```{r fisher functions}
fisher_test_matrixes
FDR_fisher
annotation_terms
```

```{r fisher test}
# creation of three sub-matrices in function of the GOterm's type  
matrixesCC <- matrixes[which(matrixes[,1]=="CC"),]
indexCC<-which(matrixes[,1]=="CC")
terms_CC<-terms[indexCC]
colnames(matrixesCC)<-c("type","a","b","c","d")
matrixesBP <- matrixes[which(matrixes[,1]=="BP"),]
indexBP<-which(matrixes[,1]=="BP")
terms_BP<-terms[indexBP]
colnames(matrixesBP)<-c("type","a","b","c","d")
matrixesMF <- matrixes[which(matrixes[,1]=="MF"),]
indexMF<-which(matrixes[,1]=="MF")
terms_MF<-terms[indexMF]
colnames(matrixesMF)<-c("type","a","b","c","d")

pval_fisherCC<-fisher_test_matrixes(matrixesCC)
pval_fisherBP<-fisher_test_matrixes(matrixesBP)
pval_fisherMF<-fisher_test_matrixes(matrixesMF)

# correction for multiple testing in fisher per BP

fisher_analysis_BP<-FDR_fisher(pval_fisherBP,terms_BP)
number_terms_annotatedBP<-length(fisher_analysis_BP[[1]])

fisher_analysis_CC<-FDR_fisher(pval_fisherCC,terms_CC)
number_terms_annotatedCC<-length(fisher_analysis_CC[[1]])

fisher_analysis_MF<-FDR_fisher(pval_fisherMF,terms_MF)
number_terms_annotatedMF<-length(fisher_analysis_MF[[1]])

# ATTENIONE: NON BISOGNA AVERE DPLYR IN LIBRERIA! (SOVRASCRIVE SELECT)
vals = select(GO.db, keys(GO.db, "GOID"), c("TERM", "ONTOLOGY"))

annotation_terms_BP<-as.data.frame(annotation_terms(vals, terms_BP))
colnames(annotation_terms_BP)<-"terms"
annotation_terms_CC<-as.data.frame(annotation_terms(vals, terms_CC))
colnames(annotation_terms_CC)<-"terms"
annotation_terms_MF<-as.data.frame(annotation_terms(vals, terms_MF))
colnames(annotation_terms_MF)<-"terms"
```

<div style="text-align: justify">
Just some outputs of the annotation process; these annotated terms are some of the terms linked to tumor growth.
</div>

```{r extract_tumor_terms function}
extract_tumor_terms
```

```{r annotation outputs}
to_viewBP<-extract_tumor_terms(annotation_terms_BP)
to_viewCC<-extract_tumor_terms(annotation_terms_CC)
to_viewMF<-extract_tumor_terms(annotation_terms_MF)
minimum<-min(length(to_viewBP), length(to_viewCC), length(to_viewMF))

dt <- cbind(to_viewBP[1:minimum],to_viewMF[1:minimum], to_viewCC[1:minimum] )
#rownames(dt)<-c('BP', 'MF', 'CC')
colnames(dt)<-c('BP','MF', 'CC')
kbl(dt) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

# Fourth Part 

## [Note] Correcting for length of genes before clustering

<div style="text-align: justify">
We correct our data for the length of genes to perform a better clustering analysis and to not have a bias in the results do to the lengths. To do it we extracted the gene lengths with the 'getGeneLengthAndGCContent' function and then divide each value for its length. We also decided to consider as one sample all those samples taken from the same subject in the same group, as we did for testing the DE genes.
</div>

```{r init clustering}
d <- getGeneLengthAndGCContent(ID_genes_selected, "hsa")
#d[[1]] has the length of all genes selected

# remove the first column of data (gene names), extract the selected genes and normalize them
data_normalized <- samples
data_normalized <- data_normalized[index_genes_selected,]
data_normalized <- t(t(data_normalized)/d[[1]])

# remove the duplicates and bind them
dataNorm_nodup_control <- remove_duplicates(data_normalized,control) 
dataNorm_nodup_disease <- remove_duplicates(data_normalized,disease) 
dataNorm_clustering <- cbind(dataNorm_nodup_control,dataNorm_nodup_disease)
```

## Cluster the genes you have selected in exercise 2 using k-means and hierarchical clustering and compare the results you obtain

<div style="text-align: justify">
We defined a grid of possible values for the number of clusters and for each of them we implement the clustering, with the kmeans function of R. For THIS function, we used as parameters iter.max=100 to avoid a too long computation or an overfitting of the data and nstart=100 to have multiple startings that lead to better results trying to avoid local minimum of the cost function. 
For the hierarchical cluster, we used the hclust function of R, with as parameter method="ward.D2": we chose ward.D2 as a method of clustering because it uses the euclidean distance and in this way we can perform a better comparison with k-means, which also uses euclidean distance. Once the entire hierarchical tree is build, we cut the tree in order to obtain different numbers of clusters (inside the for cycle).
In each iteration on possible values of k = number of clusters, we calculate also the Silhouette value.
</div>

```{r values of K}
K <- seq(1,10)
```

```{r clustering genes with kmeans}
# Clustering genes with kmeans

WITHIN_SS_gene_kmeans <- NULL
clus_km <- NULL
s_genes_kmeans <- NULL

for(i in K){
  k <- K[i]
  cl_kmeans_genes <- kmeans(x=dataNorm_clustering,centers=k,iter.max=100,nstart=3)
  clus_km <- c(clus_km,cl_kmeans_genes)
  WITHIN_SS_gene_kmeans <- rbind(WITHIN_SS_gene_kmeans, cl_kmeans_genes$tot.withinss)
  s_genes_kmeans <- rbind(s_genes_kmeans, silhouette(dataNorm_clustering,cl_kmeans_genes[[1]], k))
}

plot(K, WITHIN_SS_gene_kmeans, main = 'K-Means over genes')
```

```{r clustering genes with hierarchical}
# Clustering genes with hierarchical

D <- dist(dataNorm_clustering) 
cl_hclust_ward <- hclust(d=D, method="ward.D2")
plot(cl_hclust_ward, hang=-1, main = 'Hierarchical clustering over genes') 

s_genes_hierar <- NULL

for (i in (1:length(K))){
  k <- K[i]
  clusters_hclust_ward <- cutree(cl_hclust_ward, k=k)
  s_genes_hierar <- c(s_genes_hierar, silhouette(dataNorm_clustering,clusters_hclust_ward,k))
}
```

## Cluster the samples in your dataset (considering only the genes you have selected in exercise 2) using k-means and hierarchical clustering and compare the results you obtain

<div style="text-align: justify">
All the implementation and the parameters are equal to the implementation above. We only had to consider the transpose of the initial data matrix to cluster the samples instead of the genes.
</div>

```{r clustering sample with kmeans}
# Clustering samples with kmeans

WITHIN_SS_sample <- NULL
clus_km_sample <- NULL
s_samples_kmeans <- NULL

for(i in K) {
  k<-K[i]
  cl_kmeans_samples<-kmeans(x=t(dataNorm_clustering),centers=k,iter.max=100,nstart=100)
  clus_km_sample<-c(clus_km_sample,cl_kmeans_samples)
  WITHIN_SS_sample<-rbind(WITHIN_SS_sample, cl_kmeans_samples$tot.withinss)
  s_samples_kmeans <- rbind(s_samples_kmeans, silhouette(t(dataNorm_clustering),cl_kmeans_samples[[1]], k))
}

plot(K, WITHIN_SS_sample, main = 'K-Means over samples')
```

```{r clustering sample with hierarchical}
# Clustering samples with hierarchical

D<-dist(t(dataNorm_clustering)) 
cl_hclust_ward_S<-hclust(d=D,method="ward.D2")
plot(cl_hclust_ward_S, hang=-1, main = 'Hierarchical over samples') 

s_samples_hierar <- NULL

for (i in (1:length(K))){
  k <- K[i]
  clusters_hclust_ward_S<-cutree(cl_hclust_ward_S, k=k)
  s_samples_hierar <- c(s_samples_hierar, silhouette(t(dataNorm_clustering),clusters_hclust_ward_S,k))
}
```

## Choose the optimal number of clusters using the silhouette statistic

```{r silhouette implementation}
silhouette
```

```{r results of the sihouette}
# results for silhouette
{# HIDE
cat("K-Means over genes!\n")
#cat(s_genes_kmeans)
cat("The maximum silhouette is", max(s_genes_kmeans), ", obtained for the optimal number of clusters:", K[which(s_genes_kmeans == max(s_genes_kmeans))])

cat("\n\nHierarchical clustering over genes!\n")
#cat(s_genes_hierar)
cat("The maximum silhouette is", max(s_genes_hierar), ", obtained for the optimal number of clusters:", K[which(s_genes_hierar == max(s_genes_hierar))])

cat("\n\nK-Means over samples!\n")
#cat(s_samples_kmeans)
cat("The maximum silhouette is", max(s_samples_kmeans), ", obtained for the optimal number of clusters:", K[which(s_samples_kmeans == max(s_samples_kmeans))])

cat("\n\nHierarchical clustering over samples!\n")
#cat(s_samples_hierar)
cat("The maximum silhouette is", max(s_samples_hierar), ", obtained for the optimal number of clusters:", K[which(s_samples_hierar == max(s_samples_hierar))])
}# HIDE
```

## Choose the optimal number of clusters using the Gap statistic. In this case you can use the R function clusGap() in the library “cluster”

<div style="text-align: justify">
To implement the gap statistic, we used the clusGap function of the cluster library. For each analysis we implemented before, we determined the best number of clusters with its definition. 
</div>

```{r gap}
# results for the gap statistic

#functions for the subsequent analysis
test_hclust <- function(x, k) list(cluster=cutree(hclust(dist(x), method = "average"),k=k))
test_kmeans <- function(x, k) (kmeans(x=x,centers=k,iter.max=100,nstart=100))

# K-Means - genes

gap_res<-clusGap(dataNorm_clustering, test_kmeans, length(K), B=20)

for (i in (2:(nrow(gap_res[[1]])-1))){
  if ((gap_res[[1]][i,3])>(gap_res[[1]][i+1,3]+gap_res[[1]][i+1,4]))
    break;
}
kopt_genes_kmeans <- i

# K-Means - samples

gap_res<-clusGap(t(dataNorm_clustering), test_kmeans, length(K), B=20)

for (i in (2:(nrow(gap_res[[1]])-1))){
  if ((gap_res[[1]][i,3])>(gap_res[[1]][i+1,3]+gap_res[[1]][i+1,4]))
    break;
}
kopt_samples_kmeans <- i

# Hierarchical - genes

gap_res<-clusGap(dataNorm_clustering, test_hclust, length(K), B=20)

for (i in (2:(nrow(gap_res[[1]])-1))){
  if (gap_res[[1]][i,3]>gap_res[[1]][i+1,3]+gap_res[[1]][i+1,4])
    break;
}
kopt_genes_hier <- i

# Hierarchical - samples

gap_res <-clusGap(t(dataNorm_clustering), test_hclust, length(K), B=20)

for (i in (2:(nrow(gap_res[[1]])-1))){
  if (gap_res[[1]][i,3]>gap_res[[1]][i+1,3]+gap_res[[1]][i+1,4])
    break;
}
kopt_samples_hier <- i
```

```{r results of the gap statistic}
{# HIDE
# results for gap statistic
cat("K-Means over genes!\n")
cat("The optimal number of clusters is:", kopt_genes_kmeans)

cat("\n\nHierarchical clustering over genes!\n")
cat("The optimal number of clusters is:", kopt_samples_kmeans)

cat("\n\nK-Means over samples!\n")
cat("The optimal number of clusters is:", kopt_genes_hier)

cat("\n\nHierarchical clustering over samples!\n")
cat("The optimal number of clusters is:", kopt_samples_hier)
}# HIDE
```

<div style="text-align: justify">
Plots of the clusterings are shown, for both kmeans and hieararchical clustering. 
</div>

```{r plots of part4, HIERARCHICAL}
# GENES, HIERARCHICAL CLUSTERING
nodePar <- list(lab.cex = 0.6, pch = NA)
hcd_G <- as.dendrogram(cl_hclust_ward)

#par(mfrow=c(3,1))
hcd<-as.dendrogram(cl_hclust_ward)
plot(cut(hcd, h=400)$upper, main="Upper tree of cut at h=400", nodePar = nodePar,
     leaflab = "textlike")

plot(cut(hcd, h=400)$lower[[2]], nodePar = nodePar, leaflab = "none",
     main="Second branch of lower tree with cut at h=400")

# SAMPLES, HIERARCHICAL CLUSTERING
hcd_S <- as.dendrogram(cl_hclust_ward_S)
plot(hcd_S, type = "rectangle", horiz = TRUE, xlab = "Height", nodePar = nodePar, leaflab = "none", main="Hierarchical clustering of samples")
```

```{r plot of part 4, kmeans}
#GENES, KMEANS
cl_genes<-kmeans(x=dataNorm_clustering, centers = kopt_genes_kmeans, iter.max = 100, nstart = 100)
fviz_cluster(cl_genes, data = dataNorm_clustering,
             palette = c("#2E9FDF", "#00AFBB"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw(),
             main="Kmeans clustering of genes"
)

#SAMPLES, KMEANS
cl_samples<-kmeans(x=t(dataNorm_clustering), centers = kopt_samples_kmeans, iter.max = 100, nstart = 100)
fviz_cluster(cl_samples, data = t(dataNorm_clustering),
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw(),
             main="Kmeans clustering of samples"
)
```

# Fifth Part

## Considering the two groups you have used for differential expression analysis split your data in training and test

<div style="text-align: justify">
To build the dataset for the SVM analysis, we considered the selected genes and the two groups (control and disease seen above) and we did some pre-processing: first of all, duplicates in samples are removed with the function implemented and explained above. We then removed also the genes for which the expression values are always zero in both the groups. This will determine the data to use for SVM classification, divided then into training and test set randomly, saving the labels needed for the classification (the belonging to the control or to the disease group of each sample). 
For the subsequent implementation, we had to remove also the genes for which the expression is zero in all samples in control or all samples in disease. When taking away said genes, they must be taken away also in the other set. They won't be of any use in the classification process; in fact, in the pre-processing of data, mean and standard deviations must be computed to be used for z-scoring said data; if a gene has always expression zero in all samples, its standard deviation will also be zero. Dividing by zero is not possible, so NA will be inserted in that column; the training will then be stopped because NA will be found. For this reason, we implemented a function to remove those genes, in a very similar fashion to the function for removing zeros explained above.
</div>

```{r function remove_zeros_onegroup}
remove_zeros_onegroup 
```

```{r data splitting}
set.seed(3738)

# extract data, remove duplicates and zeros
data_nodup_control <- remove_duplicates(samples[index_genes_selected,], control) 
data_nodup_disease <- remove_duplicates(samples[index_genes_selected,], disease)
groups_nozero <- remove_zeros(data_nodup_control, data_nodup_disease)
data_nodup_control <- groups_nozero$control
data_nodup_disease <- groups_nozero$disease
data_SVM <- cbind(data_nodup_control, data_nodup_disease)
# assign its ID to each gene
rownames(data_SVM) <- ID_genes_selected[-(groups_nozero$removedindexes)]
# construct the vector for control and disease 
namegroup <- c(rep("control", ncol(data_nodup_control)),rep("disease", ncol(data_nodup_disease)))

# separate in train and test, both data and labels
trainIndex <- createDataPartition((1:ncol(data_SVM)), p=0.7, list=FALSE, times=1)
data_train <- data_SVM[,trainIndex]
data_test <- data_SVM[,-trainIndex]

label_train <- namegroup[trainIndex]
label_test <- namegroup[-trainIndex]

# we can still have genes in data_train and data_test for which all values are 0: we remove these genes (if in one group it generates this problem, we have to remove that gene also from the other group). If we do the other "normalization" we can eliminate the second three lines.
train_removed <- remove_zeros_onegroup(data_train)
data_train <- train_removed$group
data_test <- data_test[-train_removed$removedindexes,]

test_removed <-remove_zeros_onegroup(data_test)
data_test <- test_removed$group
data_train <- data_train[-test_removed$removedindexes,]

# finally, take the transpose to have samples on the rows
data_test<-t(data_test)
data_train<-t(data_train)
```

## Standardize each feature in your dataset 

<div style="text-align: justify">
The dataset contains all data of the control and disease groups only for the genes selected with the differential expression analysis. To standardize each features in the dataset (so to have mean=0 and sd=1), we used the scale function. Z-scoring is implemented: to z-score the data set, mean and standard deviation of the training set is used. We can perform two different normalizations: in the first, we standardize training and test sets with their own mean and standard deviation; in the second, we standardize both of them with the values of the training set. In this second way, we are forcing that bot sets have the same distribution.
In the second part, we add at the top of the dataframes containing the training set and the test set the label of each sample, inserting them as a factor variable.
</div>

```{r data preprocessing}
dataNorm_train <- scale(data_train)

mean_train <- apply(data_train,2,mean)
sd_train <- apply(data_train,2,sd)
dataNorm_test <- scale(data_test, center = mean_train, scale = sd_train)

# create data.frames in which in the first element we have the factor element
dataNorm_train<-as.data.frame(dataNorm_train)
dataNorm_train<-cbind('Group'=factor(label_train),dataNorm_train)

dataNorm_test<-as.data.frame(dataNorm_test)
dataNorm_test<-cbind('Group'=factor(label_test),dataNorm_test)
```

## Use linear support vector machines to learn classifying your subjects in the two classes 

<div style="text-align: justify">
We implemented the SVM procedure with both Caret and e1071 packages and we printed some results. We can see that the results for the two methods are equal. With caret, we set the model to predict the Group variable in the normalized dataset with a linear SVM, without performing any scaling (it has been already performed) and trying to maximize the accuracy of the model thanks to a repeated cross validation procedure. With e1071, we set at most the same parameters to fit a linear model for classification.
</div>

```{r caret}
# train the model
model <- train(Group~., data=dataNorm_train, method = "svmLinear", scale = FALSE, na.action=na.fail, metric = "Accuracy", maximize = TRUE, trControl = trainControl(method = "repeatedcv", number =10, repeats = 3))
summary(model)

# only some stupid results
cat("Prediction for the training set:\n")
train_pred <- predict(model,dataNorm_train)
confusionMatrix(train_pred, factor(label_train))

# test the model on the test set
cat("Prediction for the test set:\n")
test_pred <- predict(model,dataNorm_test)
confusionMatrix(test_pred, factor(label_test))
```

```{r e1071}
svmfit <- svm(Group~., data = dataNorm_train, kernel = "linear", type = 'C-classification', scale = FALSE, cross = 5)
print(svmfit)
{# HIDE
cat("Prediction for the test set:\n")
test_pred <- predict(svmfit, newdata=dataNorm_test, decision.values = FALSE)
res <- confusionMatrix(test_pred, factor(label_test))
print(res)
}# HIDE
```

## Feature selection

RFE without k-fold cross validation
```{r implemented rfe function}
recursiveFeatureExtraction
```

```{r fun}
rfe <- recursiveFeatureExtraction(dataNorm_train, label_train, dataNorm_test, label_test, 500)
```

RFE with k-fold cross validation
```{r implemented rfe function CV}
recursiveFeatureExtractionCV
```

```{r funCV}
rfe <- recursiveFeatureExtractionCV(dataNorm_train, label_train, 500, 5)
```

```{r tests of rfe}
best_genes <- (rfe$bestmodel)$names
best_model <- (rfe$bestmodel)$svm
matrix_train <- dataNorm_train[,n]
matrix_train <- cbind('Group'=factor(label_train),matrix_train)

# Performances on the test set 
pred_prova <- predict(best_model, newdata=dataNorm_test, decision.values = FALSE)
res_prova <- confusionMatrix(pred_prova, factor(label_test))  
print(res_prova)

# Check the performances
# svmfit_prova <- svm(Group ~ ., data = matrix_train, kernel = "linear", type = 'C-classification', scale = FALSE, na.action = na.omit)
# pred_prova <- predict(svmfit_prova, newdata=dataNorm_test, decision.values = FALSE)
# res_prova <- confusionMatrix(pred_prova, factor(label_test))  
# g1 <- res_prova[["overall"]][["Accuracy"]]
```

```{r svm graph}
w <- t(best_model$coefs) %*% best_model$SV
w<-abs(w)
names_sorted <- best_genes[order(w, decreasing=TRUE)]

plot(best_model, dataNorm_test, ENSG00000105464 ~ ENSG00000236533)
```
